#jinja2: trim_blocks: "true", lstrip_blocks: "true"
# This file is managed by ansible, don't make changes here - they will be overwritten.

[global]
{% if ceph.cephx %}
  auth cluster required = cephx
  auth service required = cephx
  auth client required = cephx
  cephx require signatures = {{ ceph.cephx_require_signatures }} # Kernel RBD does NOT support signatures!
  cephx cluster require signatures = {{ ceph.cephx_cluster_require_signatures }}
  cephx service require signatures = {{ ceph.cephx_service_require_signatures }}
{% else %}
  auth cluster required = none
  auth service required = none
  auth client required = none
  auth supported = none
{% endif %}
  fsid = {{ fsid_file.content | b64decode }}
  max open files = {{ ceph.max_open_files }}
  osd pool default size = {{ ceph.pool_default_size }}
  osd pool default min size = {{ ceph.pool_default_min_size }}
  osd pool default crush rule = {{ ceph.pool_default_crush_rule }}
  ms tcp rcvbuf = {{ ceph.ms_tcp_rcvbuff }}
{% if groups['ceph_osds'][1] is not defined %}
  osd crush chooseleaf type = 0
{% endif %}
{% if ceph.disable_in_memory_logs %}
  # Disable in-memory logs
  debug_lockdep = 0/0
  debug_context = 0/0
  debug_crush = 0/0
  debug_buffer = 0/0
  debug_timer = 0/0
  debug_filer = 0/0
  debug_objecter = 0/0
  debug_rados = 0/0
  debug_rbd = 0/0
  debug_journaler = 0/0
  debug_objectcatcher = 0/0
  debug_client = 0/0
  debug_osd = 0/0
  debug_optracker = 0/0
  debug_objclass = 0/0
  debug_filestore = 0/0
  debug_journal = 0/0
  debug_ms = 0/0
  debug_monc = 0/0
  debug_tp = 0/0
  debug_auth = 0/0
  debug_finisher = 0/0
  debug_heartbeatmap = 0/0
  debug_perfcounter = 0/0
  debug_asok = 0/0
  debug_throttle = 0/0
  debug_mon = 0/0
  debug_paxos = 0/0
  debug_rgw = 0/0
{% endif %}
{% if ceph.enable_debug_global %}
  debug ms = {{ ceph.debug_global_level }}
{% endif %}

[client]
  rbd cache = {{ ceph.rbd_cache }}
  rbd cache writethrough until flush = true
  rbd concurrent management ops = {{ ceph.rbd_concurrent_management_ops }}
  log file = {{ ceph.rbd_client_log_file }} # must be writable by QEMU and allowed by SELinux or AppArmor
  rbd default map options = {{ ceph.rbd_default_map_options }}
  rbd default features = {{ ceph.rbd_default_features }} # sum features digits
  rbd default format = {{ ceph.rbd_default_format }}

[mon]
  mon osd down out interval = {{ ceph.mon_osd_down_out_interval }}
  mon clock drift allowed = {{ ceph.mon_clock_drift_allowed }}
  mon clock drift warn backoff = {{ ceph.mon_clock_drift_warn_backoff }}
  mon osd full ratio = {{ ceph.mon_osd_full_ratio }}
  mon osd nearfull ratio = {{ ceph.mon_osd_nearfull_ratio }}
  mon osd report timeout = {{ ceph.mon_osd_report_timeout }}
  mon pg warn max per osd = {{ ceph.mon_pg_warn_max_per_osd }}
  mon osd allow primary affinity = {{ ceph.mon_osd_allow_primary_affinity }}
  mon warn on legacy crush tunables = {{ ceph.mon_warn_on_legacy_crush_tunables }}

{% if ceph.enable_debug_mon %}
  debug mon = {{ ceph.debug_mon_level }}
  debug paxos = {{ ceph.debug_mon_level }}
  debug auth = {{ ceph.debug_mon_level }}
{% endif %}
{% for host in groups['ceph_monitors'] %}
  {% if hostvars[host]['ansible_hostname'] is defined %}
  [mon.{{ hostvars[host]['ansible_hostname'] }}]
    host = {{ hostvars[host]['ansible_hostname'] }}
    mon addr = {{ hostvars[host]['ansible_' + ceph.monitor_interface]['ipv4']['address'] }}
  {% endif %}
{% endfor %}

[osd]
  osd mkfs type = {{ ceph.osd_mkfs_type }}
  osd mkfs options xfs = {{ ceph.osd_mkfs_options_xfs }}
  osd mount options xfs = {{ ceph.osd_mount_options_xfs }}
  osd journal size = {{ ceph.journal_size }}
{% if undercloud_cidr is defined %}
{% macro cluster_nw() %}
{% for subnet in undercloud_cidr %}
{%- if loop.last -%}
{{ subnet.cidr }}
{%- else -%}
{{ subnet.cidr }},
{%- endif -%}
{% endfor %}
{% endmacro %}
  public_network = {{ cluster_nw() }}
  cluster_network = {{ cluster_nw() }}
{% endif %}
  osd mon heartbeat interval = {{ ceph.osd_mon_heartbeat_interval }}
  osd backfill full ratio = {{ ceph.osd_backfill_full_ratio }}
  # Performance tuning
  filestore merge threshold = {{ ceph.filestore_merge_threshold }}
  filestore split multiple = {{ ceph.filestore_split_multiple }}
  osd op threads = {{ ceph.osd_op_threads }}
  filestore op threads = {{ ceph.filestore_op_threads }}
  filestore max sync interval = {{ ceph.filestore_max_sync_interval }}
  osd max scrubs = {{ ceph.osd_max_scrubs }}
  # Recovery tuning
  osd recovery max active = {{ ceph.osd_recovery_max_active }}
  osd max backfills = {{ ceph.osd_max_backfills }}
  osd recovery op priority = {{ ceph.osd_recovery_op_priority }}
  osd recovery max chunk = {{ ceph.osd_recovery_max_chunk }}
  osd recovery threads = {{ ceph.osd_recovery_threads }}
  osd objectstore = {{ ceph.osd_objectstore }}
  osd crush update on start = {{ ceph.osd_crush_update_on_start }}
{% if ceph.enable_debug_osd %}
  debug osd = {{ ceph.debug_osd_level }}
  debug filestore = {{ ceph.debug_osd_level }}
  debug journal = {{ ceph.debug_osd_level }}
  debug monc = {{ ceph.debug_osd_level }}
{% endif %}
  # Deep scrub impact
  osd scrub sleep = {{ ceph.osd_scrub_sleep }}
  osd disk thread ioprio class = {{ ceph.osd_disk_thread_ioprio_class }}
  osd disk thread ioprio priority = {{ ceph.osd_disk_thread_ioprio_priority }}
  osd scrub chunk max = {{ ceph.osd_scrub_chunk_max }}
  osd deep scrub stride = {{ ceph.osd_deep_scrub_stride }}
