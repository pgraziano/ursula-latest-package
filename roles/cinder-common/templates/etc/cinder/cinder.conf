[DEFAULT]
debug = {{ cinder.logging.debug }}
verbose = {{ cinder.logging.verbose }}
{% if cinder.auditing.enabled|bool and cinder.auditing.logging|bool %}
# Store pyCADF audit events in log #
notification_driver = log
{% endif %}

osapi_volume_workers = {{ cinder.api_workers }}
osapi_volume_listen_port={{ endpoints.cinder.port.backend_api }}

# Logging #
log_dir = /var/log/cinder
use_syslog = False
syslog_log_facility = LOG_LOCAL0
use_stderr = false

state_path = {{ cinder.state_path }}
rootwrap_config=/etc/cinder/rootwrap.conf
api_paste_config = /etc/cinder/api-paste.ini

volume_name_template = volume-%s
auth_strategy = keystone

{% if cinder.allow_availability_zone_fallback is defined -%}
allow_availability_zone_fallback = {{ cinder.allow_availability_zone_fallback }}
{% endif -%}

{% if cinder.volume_clear_size is defined -%}
volume_clear_size = {{ cinder.volume_clear_size }}
{% endif -%}

glance_host = {{ endpoints.main }}
glance_port = {{ endpoints.glance.port.backend_api }}

{% if swift.enabled|bool -%}
backup_driver = cinder.backup.drivers.swift
backup_swift_url = http://{{ endpoints.swift.haproxy_vip }}:{{ endpoints.swift.port.cinder_backup }}/v1/
{% endif -%}

enabled_backends =
{%- for backend, enabled in [('lvm', lvm.enabled),
                            ('rbd_hybrid', ceph.enabled and ceph_pools.rbd_hybrid.enabled),
                            ('rbd_ssd', ceph.enabled and ceph_pools.rbd_ssd.enabled)] if enabled -%}
{{ backend + ',' }}
{%- endfor -%}

{%- if v7k.enabled|bool -%}
     {%- for pool_name in v7k.storage_pools -%}
       {{ pool_name + ',' }}
     {%- endfor %}
 {%- endif %}

{% if cinder.default_volume_type | default(false) -%}
default_volume_type = {{ cinder.default_volume_type }}
{% elif ceph.enabled|bool %}
default_volume_type = {{ceph_pools[ceph_default_backend]['volume_type'] }}
{% elif v7k.enabled|bool -%}
{% for pool_name in v7k.storage_pools %}
{% if loop.first %}
default_volume_type = {{ pool_name|upper }}
{% endif %}
{% endfor %}
{% elif lvm.enabled|bool %}
default_volume_type ='LVM'
{% endif %}


{% if lvm.enabled -%}
[lvm]
volume_backend_name = lvm
volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver
iscsi_helper=tgtadm
iscsi_ip_address = {{ primary_ip }}
volume_group = cinder-volumes
use_forwarded_for = true
{% endif %}


{% if v7k.enabled|bool %}
{% for pool_name in v7k.storage_pools %}
[{{ pool_name }}]
host=v7k
volume_driver = cinder.volume.drivers.ibm.storwize_svc.storwize_svc_iscsi.StorwizeSVCISCSIDriver
san_ip ={{ v7k.management_ip }}
san_ssh_port={{ v7k.ssh_port }}
san_login = {{ v7k.users.cinder.name }}
san_private_key = /etc/cinder/v7k/ssh/cinder/id_rsa
storwize_svc_volpool_name = {{ pool_name }}
volume_backend_name = {{pool_name }}

{% endfor %}
{% endif %}

{% if ceph.enabled|bool -%}

{% for key, value in ceph_pools.iteritems() %}
{% if value.enabled %}
[{{ key }}]
{% if cinder.common_volume_name -%}
host = ceph
{% endif -%}
volume_backend_name = {{ key }}
volume_driver = cinder.volume.drivers.rbd.RBDDriver
rbd_pool = {{ value.pool_name }}
rbd_user = {{ ceph.cinder.rbd_user }}
rbd_secret_uuid = {{ cinder_uuid_file.content | b64decode }}
rbd_ceph_conf = {{ ceph.cinder.rbd_ceph_conf }}
rbd_flatten_volume_from_snapshot = {{ ceph.cinder.rbd_flatten_volume_from_snapshot }}
rbd_max_clone_depth = {{ ceph.cinder.rbd_max_clone_depth }}
glance_api_version = {{ ceph.cinder.glance_api_version }}
{% for backend in cinder.backends %}
{% if backend.name == key %}
{% if backend.volume_group is defined %}
volume_group = {{ backend.volume_group }}
{% endif -%}
{% if backend.san_ip is defined -%}
san_ip = {{ backend.san_ip }}
san_login = {{ backend.san_login }}
san_password = {{ backend.san_password }}
{% if backend.nimble_subnet_label is defined %}
nimble_subnet_label = {{ backend.nimble_subnet_label }}
{% endif %}
{% endif %}
{% endif %}
{% endfor -%}
{% endif %}
{% endfor -%}

{% endif -%}


[oslo_concurrency]
# If on a host that runs compute, a shared lock path is needed between
# nova and cinder for os-brick reasons. So we always use a shared location
lock_path = {{ state_path_base }}/sharedlock

[oslo_messaging_rabbit]
{% macro rabbitmq_hosts() -%}
{% for host in groups['controller'] -%}
   {% if loop.last -%}
{{ hostvars[host][primary_interface]['ipv4']['address'] }}:{{ rabbitmq.port }}
   {%- else -%}
{{ hostvars[host][primary_interface]['ipv4']['address'] }}:{{ rabbitmq.port }},
   {%- endif -%}
{% endfor -%}
{% endmacro -%}

heartbeat_timeout_threshold = {{ cinder.heartbeat_timeout_threshold }}
{% if rabbitmq.cluster -%}
rabbit_hosts = {{ rabbitmq_hosts() }}
{% else -%}
rabbit_host = {{ endpoints.rabbit }}
rabbit_port = 5672
{% endif -%}
rabbit_userid = {{ rabbitmq.user }}
rabbit_password = {{ secrets.rabbit_password }}

[oslo_middleware]
enable_proxy_headers_parsing = True

[database]
connection=mysql+pymysql://cinder:{{ secrets.db_password }}@{{ endpoints.db }}/cinder?charset=utf8

[keystone_authtoken]
identity_uri = {{ endpoints.identity_uri }}
auth_uri = {{ endpoints.auth_uri }}
admin_tenant_name = service
admin_user = cinder
admin_password = {{ secrets.service_password }}
signing_dir = /var/cache/cinder/api
cafile = {{ cinder.cafile }}
memcached_servers = {{ hostvars|ursula_memcache_hosts(groups, memcached.port) }}

[cors]
allow_headers = X-Auth-Token,X-Identity-Status,X-Roles,X-Service-Catalog,X-User-Id,X-Tenant-Id,X-OpenStack-Request-ID,X-Trace-Info,X-Trace-HMAC,OpenStack-Volume-microversion
expose_headers = X-Auth-Token,X-Subject-Token,X-Service-Token,X-OpenStack-Request-ID,OpenStack-Volume-microversion
allow_methods = GET,PUT,POST,DELETE,PATCH
